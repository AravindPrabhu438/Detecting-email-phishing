# -*- coding: utf-8 -*-
"""Untitled51_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VVUEK9medaaKsLSKiQpzl8LOu7HLnXUf
"""

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding, LSTM
from keras.layers import Conv1D, MaxPooling1D, Flatten
from matplotlib import pyplot

data = pd.read_csv('emails.csv')
data.shape

def get_process(texts, tokenizer, train=True, max_seq_length=None):

  sequences = tokenizer.texts_to_sequences(texts)
  if train==True:
    max_seq_length = np.max(list(map(lambda x: len(x), sequences)))

  sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_length, padding='post')

  return sequences

def preprocess_inputs(df):
  df = df.copy()
  
  x = df['text']
  y = df['spam']

  x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.7, shuffle=True, random_state=1)

  tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=30000, 
                                                    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                                                    lower=True)
  tokenizer.fit_on_texts(x_train)

  x_train = get_process(x_train, tokenizer, train=True)
  x_test = get_process(x_test, tokenizer, train=False, max_seq_length=x_train.shape[1])

  return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = preprocess_inputs(data)
x_train.shape

# max_seq_length = 7349
# inputs = tf.keras.Input(shape=(max_seq_length))
# embedding = tf.keras.layers.Embedding(input_dim=30000, output_dim=64)(inputs)

model = Sequential()
model.add(Embedding(input_dim=30000,
                    output_dim=64,
                    input_length=7349))
model.add(Dropout(0.5))
model.add(Conv1D(filters=32,kernel_size=3,padding='same',
                 activation='relu'))
model.add(MaxPooling1D())
model.add(Conv1D(filters=16, kernel_size=3, padding='same',
                 activation='relu'))
model.add(MaxPooling1D())
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer = 'adam',
              loss = 'binary_crossentropy',
              metrics = [['accuracy'], ['Precision'], ['Recall']])

print(model.summary())

history = model.fit(x_train, y_train,
                    validation_split=0.2,
                    batch_size=32,
                    epochs=15,
                    callbacks=[
                               tf.keras.callbacks.EarlyStopping(
                                   monitor='val_loss',
                                   patience=3,
                                   restore_best_weights=True
                               )
                    ])

results = model.evaluate(x_test, y_test, verbose=0)

print(' Test loss: {:.4f}'.format(results[0]))
print('Accuracy: {:.2f}%'.format(results[1]*100))

pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='validation')
pyplot.legend()
# plot accuracy during training

pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['accuracy'], label='train')
pyplot.plot(history.history['val_accuracy'], label='validation')
pyplot.legend()
pyplot.show()

from sklearn.metrics import confusion_matrix
y_pred = model.predict(x_test)
y_pred

y_pred = model.predict_classes(x_test)
y_pred

conf_matrix = confusion_matrix(y_test, y_pred)

conf_matrix

import seaborn as sns
sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True, 
            fmt='.2%', cmap='Blues')